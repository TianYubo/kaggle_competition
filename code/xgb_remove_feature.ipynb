{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Tuple\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "import joblib\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Users/kyleee/code/project/kaggle_competition/data\")\n",
    "from data.kaggle_evaluation import jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_xgb(y_true, y_pred, sample_weight):\n",
    "   \"\"\"\n",
    "   为 XGBoost 计算加权 R2 分数的自定义评估指标\n",
    "   \n",
    "   Args:\n",
    "       y_true: 真实标签值\n",
    "       y_pred: 模型预测值  \n",
    "       sample_weight: 样本权重\n",
    "\n",
    "   Returns:\n",
    "       float: 负的 R2 分数（XGBoost 默认最小化损失，所以返回负值）\n",
    "       \n",
    "   Note:\n",
    "       R2 = 1 - weighted_mse(y_true, y_pred) / weighted_var(y_true)\n",
    "       分母加入小值 1e-38 避免除零错误\n",
    "   \"\"\"\n",
    "   r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "   return -r2\n",
    "\n",
    "def r2_lgb(y_true, y_pred, sample_weight):\n",
    "   \"\"\"\n",
    "   为 LightGBM 计算加权 R2 分数的自定义评估指标\n",
    "   \n",
    "   Args:\n",
    "       y_true: 真实标签值\n",
    "       y_pred: 模型预测值\n",
    "       sample_weight: 样本权重\n",
    "\n",
    "   Returns:\n",
    "       tuple: ('r2', r2_score, is_higher_better)\n",
    "       - 指标名称\n",
    "       - R2 分数 \n",
    "       - 是否更大的值更好\n",
    "   \"\"\"\n",
    "   r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "   return 'r2', r2, True\n",
    "\n",
    "class r2_cbt(object):\n",
    "   \"\"\"\n",
    "   为 CatBoost 计算 R2 分数的自定义评估指标类\n",
    "   \n",
    "   实现 CatBoost 自定义评估指标所需的接口:\n",
    "   - get_final_error: 计算最终的评估分数\n",
    "   - is_max_optimal: 指示是否更大的值更好\n",
    "   - evaluate: 计算误差和权重\n",
    "   \"\"\"\n",
    "   \n",
    "   def get_final_error(self, error, weight):\n",
    "       \"\"\"\n",
    "       计算最终的 R2 分数\n",
    "       \n",
    "       Args:\n",
    "           error: evaluate() 返回的误差和\n",
    "           weight: evaluate() 返回的权重和\n",
    "           \n",
    "       Returns:\n",
    "           float: R2 分数\n",
    "       \"\"\"\n",
    "       return 1 - error / (weight + 1e-38)\n",
    "\n",
    "   def is_max_optimal(self):\n",
    "       \"\"\"\n",
    "       指示是否更大的评估分数更好\n",
    "       \n",
    "       Returns:\n",
    "           bool: True 表示更大的值更好\n",
    "       \"\"\"\n",
    "       return True\n",
    "\n",
    "   def evaluate(self, approxes, target, weight):\n",
    "       \"\"\"\n",
    "       计算误差平方和与目标值平方和\n",
    "       \n",
    "       Args:\n",
    "           approxes: 预测值列表的列表\n",
    "           target: 真实标签值\n",
    "           weight: 样本权重\n",
    "           \n",
    "       Returns:\n",
    "           tuple: (error_sum, weight_sum)\n",
    "           - error_sum: 加权误差平方和\n",
    "           - weight_sum: 加权目标值平方和\n",
    "           \n",
    "       Note:\n",
    "           CatBoost 的接口要求 approxes 是预测值的列表，\n",
    "           但此实现只使用第一个预测值列表\n",
    "       \"\"\"\n",
    "       assert len(approxes) == 1\n",
    "       assert len(target) == len(approxes[0])\n",
    "\n",
    "       approx = approxes[0]\n",
    "       error_sum = 0.0  # 加权误差平方和\n",
    "       weight_sum = 0.0  # 加权目标值平方和\n",
    "\n",
    "       for i in range(len(approx)):\n",
    "           w = 1.0 if weight is None else weight[i]\n",
    "           weight_sum += w * (target[i] ** 2)\n",
    "           error_sum += w * ((approx[i] - target[i]) ** 2)\n",
    "\n",
    "       return error_sum, weight_sum\n",
    "\n",
    "def reduce_mem_usage(df: pd.DataFrame, float16_as32: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    通过调整数据类型来减少 DataFrame 的内存使用。\n",
    "    \n",
    "    Args:\n",
    "        df: 输入的 DataFrame\n",
    "        float16_as32: 是否将 float16 范围内的数据转换为 float32 以提高精度\n",
    "        \n",
    "    Returns:\n",
    "        优化内存使用后的 DataFrame\n",
    "        \n",
    "    Note:\n",
    "        数值类型转换范围：\n",
    "        - int8:   -128 到 127\n",
    "        - int16:  -32,768 到 32,767\n",
    "        - int32:  -2,147,483,648 到 2,147,483,647\n",
    "        - int64:  -9,223,372,036,854,775,808 到 9,223,372,036,854,775,807\n",
    "        - float16: ±6.55e±4\n",
    "        - float32: ±3.4e±38\n",
    "        - float64: ±1.7e±308\n",
    "    \"\"\"\n",
    "    # 计算初始内存使用量（MB）\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # 跳过非数值类型的列（对象类型和类别类型）\n",
    "        if col_type != object and str(col_type) != 'category':\n",
    "            c_min, c_max = df[col].min(), df[col].max()\n",
    "            \n",
    "            # 处理整数类型\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # 根据数据范围选择最小的可用整数类型\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            \n",
    "            # 处理浮点数类型\n",
    "            else:\n",
    "                # 根据数据范围选择最小的可用浮点类型\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    if float16_as32:\n",
    "                        # 对精度敏感的数据使用 float32\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    # 计算优化后的内存使用量和节省比例\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(\n",
    "    df: pd.DataFrame,\n",
    "    feature_names: List[str],\n",
    "    dates: np.ndarray,\n",
    "    num_valid_dates: int\n",
    ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    准备训练集和验证集的数据\n",
    "    \n",
    "    Args:\n",
    "        df: 原始数据框\n",
    "        feature_names: 特征列名列表\n",
    "        dates: 唯一日期数组\n",
    "        num_valid_dates: 验证集使用的日期数量\n",
    "    \n",
    "    Returns:\n",
    "        训练集和验证集的字典，包含特征(X)、标签(y)和权重(w)\n",
    "    \"\"\"\n",
    "    # 选择最后 num_valid_dates 个日期作为验证集, 其余作为训练集\n",
    "    # 例如，如果 num_valid_dates=10，dates=[0, 1, 2, ..., 99]，则 valid_dates=[90, 91, ..., 99]\n",
    "    valid_dates = dates[-num_valid_dates:]\n",
    "    train_dates = dates[:-num_valid_dates]\n",
    "    \n",
    "    # 准备验证集，只包含 valid_dates 中的数据\n",
    "    # X: 特征, y: 标签, w: 权重\n",
    "    valid_mask = df['date_id'].isin(valid_dates)\n",
    "    valid_data = {\n",
    "        'X': df[feature_names][valid_mask],\n",
    "        'y': df['responder_6'][valid_mask],\n",
    "        'w': df['weight'][valid_mask]\n",
    "    }\n",
    "    \n",
    "    # 准备基础训练集数据, 同上\n",
    "    train_data = {\n",
    "        'dates': train_dates,\n",
    "        'X': df[feature_names],\n",
    "        'y': df['responder_6'],\n",
    "        'w': df['weight']\n",
    "    }\n",
    "    \n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: Union[lgb.LGBMRegressor, xgb.XGBRegressor, cbt.CatBoostRegressor],\n",
    "    train_data: Dict[str, pd.DataFrame],\n",
    "    valid_data: Dict[str, pd.DataFrame],\n",
    "    model_type: str,\n",
    ") -> Union[lgb.LGBMRegressor, xgb.XGBRegressor, cbt.CatBoostRegressor]:\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    \n",
    "    Args:\n",
    "        model: 模型实例\n",
    "        train_data: 训练数据字典\n",
    "        valid_data: 验证数据字典\n",
    "        model_type: 模型类型 ('lgb', 'xgb', 或 'cbt')\n",
    "    \n",
    "    Returns:\n",
    "        训练好的模型\n",
    "    \"\"\"\n",
    "    if model_type == 'lgb':\n",
    "        model.fit(\n",
    "            train_data['X'], train_data['y'], \n",
    "            sample_weight=train_data['w'],\n",
    "            eval_metric=[r2_lgb],\n",
    "            eval_set=[(valid_data['X'], valid_data['y'], valid_data['w'])],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(100),\n",
    "                lgb.log_evaluation(10)\n",
    "            ]\n",
    "        )\n",
    "    elif model_type == 'cbt':\n",
    "        evalset = cbt.Pool(valid_data['X'], valid_data['y'], weight=valid_data['w'])\n",
    "        model.fit(\n",
    "            train_data['X'], train_data['y'],\n",
    "            sample_weight=train_data['w'],\n",
    "            eval_set=[evalset],\n",
    "            verbose=10,\n",
    "        )\n",
    "    elif model_type == 'xgb':  # xgb\n",
    "        model.fit(\n",
    "            train_data['X'], train_data['y'],\n",
    "            sample_weight=train_data['w'],\n",
    "            eval_set=[(valid_data['X'], valid_data['y'])],\n",
    "            sample_weight_eval_set=[valid_data['w']],\n",
    "            verbose=10,\n",
    "        )\n",
    "    else:\n",
    "        model = None\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "input_path = \"../data\"\n",
    "model_save_path = \"../models/feature_remove\"\n",
    "TRAINING = True\n",
    "N_fold = 5\n",
    "num_valid_dates = 10\n",
    "skip_dates = 100\n",
    "feature_nums = 79\n",
    "models_toUse = ['xgb', 'lgb', 'cbt']     # 训练的模型类型\n",
    "# 记录所有使用的模型，以及每一个模型所采取的不同配置\n",
    "model_dict = {\n",
    "    'lgb': lgb.LGBMRegressor(n_estimators=1000, \n",
    "                            device='cpu',  #! 对于苹果芯片，需要设置为 'cpu'\n",
    "                            gpu_use_dp=True, \n",
    "                            objective='l2'),\n",
    "    'xgb': xgb.XGBRegressor(n_estimators=200,\n",
    "                            learning_rate=0.2,\n",
    "                            max_depth=8,\n",
    "                            tree_method='hist',\n",
    "                            device=\"cpu\",   #! 对于苹果芯片，需要设置为 'cpu'\n",
    "                            objective='reg:squarederror',   # 目标：均方误差\n",
    "                            eval_metric=r2_xgb,\n",
    "                            disable_default_eval_metric=True,\n",
    "                            early_stopping_rounds=100,\n",
    "                            callbacks=[\n",
    "                                xgb.callback.EvaluationMonitor(show_stdv=False)  # 只显示均值，不显示标准差\n",
    "                            ]\n",
    "                        ),\n",
    "    'cbt': cbt.CatBoostRegressor(iterations=200, \n",
    "                                learning_rate=0.2, \n",
    "                                task_type='CPU',   #! 对于苹果芯片，需要设置为 'CPU'，而不是 'GPU'\n",
    "                                loss_function='RMSE',\n",
    "                                verbose=True,       # 启用详细日志\n",
    "                                metric_period=1,\n",
    "                                early_stopping_rounds=100,\n",
    "                                eval_metric=r2_cbt(),   # 设置了 eval_metric 之后，只会打印这个指标\n",
    "                                )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* 根据 features.csv，计算 correlation matrix, 筛除冗余特征\n",
    "all_feature_names = [f\"feature_{i:02d}\" for i in range(feature_nums)]     # 记录所有特征的名字，从'feature_00' 到 'feature_79'\n",
    "features_tags = pd.read_csv(f\"{input_path}/features.csv\")\n",
    "correlation_matrix = features_tags[[ f\"tag_{no}\" for no in range(0,17,1) ] ].T.corr()\n",
    "correlation_threshold = 0.8\n",
    "to_drop = set()\n",
    "fullset = set(range(feature_nums))\n",
    "# 遍历 correlation matrix，把相关性大于阈值的特征加入到 to_drop 中\n",
    "for i in range(feature_nums):\n",
    "    for j in range(i+1, feature_nums):\n",
    "        if correlation_matrix.iloc[i, j] > correlation_threshold:\n",
    "            feature_to_drop = correlation_matrix.columns[j]\n",
    "            to_drop.add(feature_to_drop)\n",
    "# 获取剩余特征的名字\n",
    "feature_names = list(fullset - to_drop)\n",
    "feature_names = [all_feature_names[i] for i in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 5595.75 MB\n",
      "Memory usage after optimization is: 2876.91 MB\n",
      "Decreased by 48.6%\n",
      "----------- Start to Load Dataset! -----------\n",
      "----------- Dataset Loaded! -----------\n",
      "[0]\tvalidation_0-r2_xgb:-0.00524\n",
      "[0]\tvalidation_0-r2_xgb:-0.00524\n",
      "[1]\tvalidation_0-r2_xgb:-0.00833\n",
      "[2]\tvalidation_0-r2_xgb:-0.01140\n",
      "[3]\tvalidation_0-r2_xgb:-0.01324\n",
      "[4]\tvalidation_0-r2_xgb:-0.01529\n",
      "[5]\tvalidation_0-r2_xgb:-0.01705\n",
      "[6]\tvalidation_0-r2_xgb:-0.01891\n",
      "[7]\tvalidation_0-r2_xgb:-0.02065\n",
      "[8]\tvalidation_0-r2_xgb:-0.02256\n",
      "[9]\tvalidation_0-r2_xgb:-0.02311\n",
      "[10]\tvalidation_0-r2_xgb:-0.02407\n",
      "[10]\tvalidation_0-r2_xgb:-0.02407\n",
      "[11]\tvalidation_0-r2_xgb:-0.02615\n",
      "[12]\tvalidation_0-r2_xgb:-0.02679\n",
      "[13]\tvalidation_0-r2_xgb:-0.02851\n",
      "[14]\tvalidation_0-r2_xgb:-0.02958\n",
      "[15]\tvalidation_0-r2_xgb:-0.03062\n",
      "[16]\tvalidation_0-r2_xgb:-0.03163\n",
      "[17]\tvalidation_0-r2_xgb:-0.03301\n",
      "[18]\tvalidation_0-r2_xgb:-0.03386\n",
      "[19]\tvalidation_0-r2_xgb:-0.03463\n",
      "[20]\tvalidation_0-r2_xgb:-0.03553\n",
      "[20]\tvalidation_0-r2_xgb:-0.03553\n",
      "[21]\tvalidation_0-r2_xgb:-0.03611\n",
      "[22]\tvalidation_0-r2_xgb:-0.03844\n",
      "[23]\tvalidation_0-r2_xgb:-0.03909\n",
      "[24]\tvalidation_0-r2_xgb:-0.04026\n",
      "[25]\tvalidation_0-r2_xgb:-0.04120\n",
      "[26]\tvalidation_0-r2_xgb:-0.04200\n",
      "[27]\tvalidation_0-r2_xgb:-0.04272\n",
      "[28]\tvalidation_0-r2_xgb:-0.04336\n",
      "[29]\tvalidation_0-r2_xgb:-0.04402\n",
      "[30]\tvalidation_0-r2_xgb:-0.04503\n",
      "[30]\tvalidation_0-r2_xgb:-0.04503\n",
      "[31]\tvalidation_0-r2_xgb:-0.04674\n",
      "[32]\tvalidation_0-r2_xgb:-0.04788\n",
      "[33]\tvalidation_0-r2_xgb:-0.04863\n",
      "[34]\tvalidation_0-r2_xgb:-0.04987\n",
      "[35]\tvalidation_0-r2_xgb:-0.05069\n",
      "[36]\tvalidation_0-r2_xgb:-0.05106\n",
      "[37]\tvalidation_0-r2_xgb:-0.05200\n",
      "[38]\tvalidation_0-r2_xgb:-0.05399\n",
      "[39]\tvalidation_0-r2_xgb:-0.05416\n",
      "[40]\tvalidation_0-r2_xgb:-0.05459\n",
      "[40]\tvalidation_0-r2_xgb:-0.05459\n",
      "[41]\tvalidation_0-r2_xgb:-0.05490\n",
      "[42]\tvalidation_0-r2_xgb:-0.05554\n",
      "[43]\tvalidation_0-r2_xgb:-0.05817\n",
      "[44]\tvalidation_0-r2_xgb:-0.05845\n",
      "[45]\tvalidation_0-r2_xgb:-0.05880\n",
      "[46]\tvalidation_0-r2_xgb:-0.05940\n",
      "[47]\tvalidation_0-r2_xgb:-0.05994\n",
      "[48]\tvalidation_0-r2_xgb:-0.06066\n",
      "[49]\tvalidation_0-r2_xgb:-0.06163\n",
      "[50]\tvalidation_0-r2_xgb:-0.06232\n",
      "[50]\tvalidation_0-r2_xgb:-0.06232\n",
      "[51]\tvalidation_0-r2_xgb:-0.06370\n",
      "[52]\tvalidation_0-r2_xgb:-0.06430\n",
      "[53]\tvalidation_0-r2_xgb:-0.06524\n",
      "[54]\tvalidation_0-r2_xgb:-0.06558\n",
      "[55]\tvalidation_0-r2_xgb:-0.06584\n",
      "[56]\tvalidation_0-r2_xgb:-0.06643\n",
      "[57]\tvalidation_0-r2_xgb:-0.06704\n",
      "[58]\tvalidation_0-r2_xgb:-0.06942\n",
      "[59]\tvalidation_0-r2_xgb:-0.07032\n",
      "[60]\tvalidation_0-r2_xgb:-0.07219\n",
      "[60]\tvalidation_0-r2_xgb:-0.07219\n",
      "[61]\tvalidation_0-r2_xgb:-0.07388\n",
      "[62]\tvalidation_0-r2_xgb:-0.07407\n",
      "[63]\tvalidation_0-r2_xgb:-0.07426\n",
      "[64]\tvalidation_0-r2_xgb:-0.07452\n",
      "[65]\tvalidation_0-r2_xgb:-0.07542\n",
      "[66]\tvalidation_0-r2_xgb:-0.07605\n",
      "[67]\tvalidation_0-r2_xgb:-0.07633\n",
      "[68]\tvalidation_0-r2_xgb:-0.07712\n",
      "[69]\tvalidation_0-r2_xgb:-0.07797\n",
      "[70]\tvalidation_0-r2_xgb:-0.07839\n",
      "[70]\tvalidation_0-r2_xgb:-0.07839\n",
      "[71]\tvalidation_0-r2_xgb:-0.07882\n",
      "[72]\tvalidation_0-r2_xgb:-0.08026\n",
      "[73]\tvalidation_0-r2_xgb:-0.08085\n",
      "[74]\tvalidation_0-r2_xgb:-0.08134\n",
      "[75]\tvalidation_0-r2_xgb:-0.08192\n",
      "[76]\tvalidation_0-r2_xgb:-0.08293\n",
      "[77]\tvalidation_0-r2_xgb:-0.08332\n",
      "[78]\tvalidation_0-r2_xgb:-0.08427\n",
      "[79]\tvalidation_0-r2_xgb:-0.08473\n",
      "[80]\tvalidation_0-r2_xgb:-0.08526\n",
      "[80]\tvalidation_0-r2_xgb:-0.08526\n",
      "[81]\tvalidation_0-r2_xgb:-0.08526\n",
      "[82]\tvalidation_0-r2_xgb:-0.08595\n",
      "[83]\tvalidation_0-r2_xgb:-0.08797\n",
      "[84]\tvalidation_0-r2_xgb:-0.08951\n",
      "[85]\tvalidation_0-r2_xgb:-0.09044\n",
      "[86]\tvalidation_0-r2_xgb:-0.09105\n",
      "[87]\tvalidation_0-r2_xgb:-0.09156\n",
      "[88]\tvalidation_0-r2_xgb:-0.09266\n",
      "[89]\tvalidation_0-r2_xgb:-0.09289\n",
      "[90]\tvalidation_0-r2_xgb:-0.09330\n",
      "[90]\tvalidation_0-r2_xgb:-0.09330\n",
      "[91]\tvalidation_0-r2_xgb:-0.09375\n",
      "[92]\tvalidation_0-r2_xgb:-0.09404\n",
      "[93]\tvalidation_0-r2_xgb:-0.09510\n",
      "[94]\tvalidation_0-r2_xgb:-0.09581\n",
      "[95]\tvalidation_0-r2_xgb:-0.09602\n",
      "[96]\tvalidation_0-r2_xgb:-0.09640\n",
      "[97]\tvalidation_0-r2_xgb:-0.09664\n",
      "[98]\tvalidation_0-r2_xgb:-0.09678\n",
      "[99]\tvalidation_0-r2_xgb:-0.09804\n",
      "[100]\tvalidation_0-r2_xgb:-0.09824\n",
      "[100]\tvalidation_0-r2_xgb:-0.09824\n",
      "[101]\tvalidation_0-r2_xgb:-0.09892\n",
      "[102]\tvalidation_0-r2_xgb:-0.09915\n",
      "[103]\tvalidation_0-r2_xgb:-0.09955\n",
      "[104]\tvalidation_0-r2_xgb:-0.09996\n",
      "[105]\tvalidation_0-r2_xgb:-0.10039\n",
      "[106]\tvalidation_0-r2_xgb:-0.10108\n",
      "[107]\tvalidation_0-r2_xgb:-0.10142\n",
      "[108]\tvalidation_0-r2_xgb:-0.10190\n",
      "[109]\tvalidation_0-r2_xgb:-0.10535\n",
      "[110]\tvalidation_0-r2_xgb:-0.10561\n",
      "[110]\tvalidation_0-r2_xgb:-0.10561\n",
      "[111]\tvalidation_0-r2_xgb:-0.10660\n",
      "[112]\tvalidation_0-r2_xgb:-0.10690\n",
      "[113]\tvalidation_0-r2_xgb:-0.10733\n",
      "[114]\tvalidation_0-r2_xgb:-0.10769\n",
      "[115]\tvalidation_0-r2_xgb:-0.10811\n",
      "[116]\tvalidation_0-r2_xgb:-0.10865\n",
      "[117]\tvalidation_0-r2_xgb:-0.10883\n",
      "[118]\tvalidation_0-r2_xgb:-0.10927\n",
      "[119]\tvalidation_0-r2_xgb:-0.10962\n",
      "[120]\tvalidation_0-r2_xgb:-0.11007\n",
      "[120]\tvalidation_0-r2_xgb:-0.11007\n",
      "[121]\tvalidation_0-r2_xgb:-0.11098\n",
      "[122]\tvalidation_0-r2_xgb:-0.11230\n",
      "[123]\tvalidation_0-r2_xgb:-0.11292\n",
      "[124]\tvalidation_0-r2_xgb:-0.11468\n",
      "[125]\tvalidation_0-r2_xgb:-0.11510\n",
      "[126]\tvalidation_0-r2_xgb:-0.11532\n",
      "[127]\tvalidation_0-r2_xgb:-0.11556\n",
      "[128]\tvalidation_0-r2_xgb:-0.11757\n",
      "[129]\tvalidation_0-r2_xgb:-0.11763\n",
      "[130]\tvalidation_0-r2_xgb:-0.11821\n",
      "[130]\tvalidation_0-r2_xgb:-0.11821\n",
      "[131]\tvalidation_0-r2_xgb:-0.11898\n",
      "[132]\tvalidation_0-r2_xgb:-0.11985\n",
      "[133]\tvalidation_0-r2_xgb:-0.12139\n",
      "[134]\tvalidation_0-r2_xgb:-0.12159\n",
      "[135]\tvalidation_0-r2_xgb:-0.12201\n",
      "[136]\tvalidation_0-r2_xgb:-0.12232\n",
      "[137]\tvalidation_0-r2_xgb:-0.12295\n",
      "[138]\tvalidation_0-r2_xgb:-0.12383\n",
      "[139]\tvalidation_0-r2_xgb:-0.12401\n",
      "[140]\tvalidation_0-r2_xgb:-0.12432\n",
      "[140]\tvalidation_0-r2_xgb:-0.12432\n",
      "[141]\tvalidation_0-r2_xgb:-0.12517\n",
      "[142]\tvalidation_0-r2_xgb:-0.12559\n",
      "[143]\tvalidation_0-r2_xgb:-0.12578\n",
      "[144]\tvalidation_0-r2_xgb:-0.12653\n",
      "[145]\tvalidation_0-r2_xgb:-0.12703\n",
      "[146]\tvalidation_0-r2_xgb:-0.12745\n",
      "[147]\tvalidation_0-r2_xgb:-0.12775\n",
      "[148]\tvalidation_0-r2_xgb:-0.12819\n",
      "[149]\tvalidation_0-r2_xgb:-0.12840\n",
      "[150]\tvalidation_0-r2_xgb:-0.12906\n",
      "[150]\tvalidation_0-r2_xgb:-0.12906\n",
      "[151]\tvalidation_0-r2_xgb:-0.12995\n",
      "[152]\tvalidation_0-r2_xgb:-0.13011\n",
      "[153]\tvalidation_0-r2_xgb:-0.13084\n",
      "[154]\tvalidation_0-r2_xgb:-0.13150\n",
      "[155]\tvalidation_0-r2_xgb:-0.13162\n",
      "[156]\tvalidation_0-r2_xgb:-0.13202\n",
      "[157]\tvalidation_0-r2_xgb:-0.13230\n",
      "[158]\tvalidation_0-r2_xgb:-0.13249\n",
      "[159]\tvalidation_0-r2_xgb:-0.13296\n",
      "[160]\tvalidation_0-r2_xgb:-0.13316\n",
      "[160]\tvalidation_0-r2_xgb:-0.13316\n",
      "[161]\tvalidation_0-r2_xgb:-0.13372\n",
      "[162]\tvalidation_0-r2_xgb:-0.13403\n",
      "[163]\tvalidation_0-r2_xgb:-0.13440\n",
      "[164]\tvalidation_0-r2_xgb:-0.13464\n",
      "[165]\tvalidation_0-r2_xgb:-0.13480\n",
      "[166]\tvalidation_0-r2_xgb:-0.13532\n",
      "[167]\tvalidation_0-r2_xgb:-0.13574\n",
      "[168]\tvalidation_0-r2_xgb:-0.13642\n",
      "[169]\tvalidation_0-r2_xgb:-0.13685\n",
      "[170]\tvalidation_0-r2_xgb:-0.13704\n",
      "[170]\tvalidation_0-r2_xgb:-0.13704\n",
      "[171]\tvalidation_0-r2_xgb:-0.13735\n",
      "[172]\tvalidation_0-r2_xgb:-0.13772\n",
      "[173]\tvalidation_0-r2_xgb:-0.13789\n",
      "[174]\tvalidation_0-r2_xgb:-0.13810\n",
      "[175]\tvalidation_0-r2_xgb:-0.13852\n",
      "[176]\tvalidation_0-r2_xgb:-0.13897\n",
      "[177]\tvalidation_0-r2_xgb:-0.13941\n",
      "[178]\tvalidation_0-r2_xgb:-0.13993\n",
      "[179]\tvalidation_0-r2_xgb:-0.14030\n",
      "[180]\tvalidation_0-r2_xgb:-0.14058\n",
      "[180]\tvalidation_0-r2_xgb:-0.14058\n",
      "[181]\tvalidation_0-r2_xgb:-0.14117\n",
      "[182]\tvalidation_0-r2_xgb:-0.14159\n",
      "[183]\tvalidation_0-r2_xgb:-0.14203\n",
      "[184]\tvalidation_0-r2_xgb:-0.14231\n",
      "[185]\tvalidation_0-r2_xgb:-0.14258\n",
      "[186]\tvalidation_0-r2_xgb:-0.14292\n",
      "[187]\tvalidation_0-r2_xgb:-0.14492\n",
      "[188]\tvalidation_0-r2_xgb:-0.14581\n",
      "[189]\tvalidation_0-r2_xgb:-0.14585\n",
      "[190]\tvalidation_0-r2_xgb:-0.14600\n",
      "[190]\tvalidation_0-r2_xgb:-0.14600\n",
      "[191]\tvalidation_0-r2_xgb:-0.14608\n",
      "[192]\tvalidation_0-r2_xgb:-0.14671\n",
      "[193]\tvalidation_0-r2_xgb:-0.14747\n",
      "[194]\tvalidation_0-r2_xgb:-0.14791\n",
      "[195]\tvalidation_0-r2_xgb:-0.14919\n",
      "[196]\tvalidation_0-r2_xgb:-0.14950\n",
      "[197]\tvalidation_0-r2_xgb:-0.15023\n",
      "[198]\tvalidation_0-r2_xgb:-0.15111\n",
      "[199]\tvalidation_0-r2_xgb:-0.15168\n",
      "[199]\tvalidation_0-r2_xgb:-0.15168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.869319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11748\n",
      "[LightGBM] [Info] Number of data points in the train set: 16575064, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score -0.004313\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's l2: 1.2841\tvalid_0's r2: 0.00774895\n",
      "[20]\tvalid_0's l2: 1.2782\tvalid_0's r2: 0.0123016\n",
      "[30]\tvalid_0's l2: 1.27485\tvalid_0's r2: 0.0148958\n",
      "[40]\tvalid_0's l2: 1.27229\tvalid_0's r2: 0.0168708\n",
      "[50]\tvalid_0's l2: 1.2698\tvalid_0's r2: 0.018798\n",
      "[60]\tvalid_0's l2: 1.26761\tvalid_0's r2: 0.0204882\n",
      "[70]\tvalid_0's l2: 1.26495\tvalid_0's r2: 0.0225423\n",
      "[80]\tvalid_0's l2: 1.26163\tvalid_0's r2: 0.0251095\n",
      "[90]\tvalid_0's l2: 1.2584\tvalid_0's r2: 0.0276087\n",
      "[100]\tvalid_0's l2: 1.25478\tvalid_0's r2: 0.0304029\n",
      "[110]\tvalid_0's l2: 1.25389\tvalid_0's r2: 0.0310908\n",
      "[120]\tvalid_0's l2: 1.25217\tvalid_0's r2: 0.0324227\n",
      "[130]\tvalid_0's l2: 1.2502\tvalid_0's r2: 0.0339424\n",
      "[140]\tvalid_0's l2: 1.24912\tvalid_0's r2: 0.0347754\n",
      "[150]\tvalid_0's l2: 1.24772\tvalid_0's r2: 0.0358591\n",
      "[160]\tvalid_0's l2: 1.24623\tvalid_0's r2: 0.0370118\n",
      "[170]\tvalid_0's l2: 1.24403\tvalid_0's r2: 0.0387101\n",
      "[180]\tvalid_0's l2: 1.24251\tvalid_0's r2: 0.0398868\n",
      "[190]\tvalid_0's l2: 1.24102\tvalid_0's r2: 0.0410348\n",
      "[200]\tvalid_0's l2: 1.23907\tvalid_0's r2: 0.0425441\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[199]\tvalid_0's l2: 1.23907\tvalid_0's r2: 0.0425451\n",
      "0:\tlearn: 0.0019021\ttest: 0.0013998\tbest: 0.0013998 (0)\ttotal: 1.74s\tremaining: 5m 45s\n",
      "10:\tlearn: 0.0090981\ttest: 0.0089071\tbest: 0.0089071 (10)\ttotal: 6.86s\tremaining: 1m 57s\n",
      "20:\tlearn: 0.0122055\ttest: 0.0121613\tbest: 0.0121613 (20)\ttotal: 11.7s\tremaining: 1m 39s\n",
      "30:\tlearn: 0.0144959\ttest: 0.0143184\tbest: 0.0143184 (30)\ttotal: 17.1s\tremaining: 1m 33s\n",
      "40:\tlearn: 0.0160705\ttest: 0.0164150\tbest: 0.0164150 (40)\ttotal: 22s\tremaining: 1m 25s\n",
      "50:\tlearn: 0.0176395\ttest: 0.0184688\tbest: 0.0184688 (50)\ttotal: 26.7s\tremaining: 1m 18s\n",
      "60:\tlearn: 0.0189938\ttest: 0.0195417\tbest: 0.0195417 (60)\ttotal: 32s\tremaining: 1m 12s\n",
      "70:\tlearn: 0.0201441\ttest: 0.0206234\tbest: 0.0206234 (70)\ttotal: 37s\tremaining: 1m 7s\n",
      "80:\tlearn: 0.0211656\ttest: 0.0228311\tbest: 0.0228311 (80)\ttotal: 42.1s\tremaining: 1m 1s\n",
      "90:\tlearn: 0.0222745\ttest: 0.0236820\tbest: 0.0236820 (90)\ttotal: 47.4s\tremaining: 56.8s\n",
      "100:\tlearn: 0.0233492\ttest: 0.0246708\tbest: 0.0246708 (100)\ttotal: 53.1s\tremaining: 52.1s\n",
      "110:\tlearn: 0.0243283\ttest: 0.0256923\tbest: 0.0256923 (110)\ttotal: 58.6s\tremaining: 47s\n",
      "120:\tlearn: 0.0253558\ttest: 0.0268191\tbest: 0.0268191 (120)\ttotal: 1m 3s\tremaining: 41.7s\n",
      "130:\tlearn: 0.0262771\ttest: 0.0272120\tbest: 0.0272120 (130)\ttotal: 1m 9s\tremaining: 36.4s\n",
      "140:\tlearn: 0.0272092\ttest: 0.0279284\tbest: 0.0279284 (140)\ttotal: 1m 14s\tremaining: 31.2s\n",
      "150:\tlearn: 0.0280555\ttest: 0.0297892\tbest: 0.0297892 (150)\ttotal: 1m 19s\tremaining: 25.8s\n",
      "160:\tlearn: 0.0289094\ttest: 0.0305851\tbest: 0.0305851 (160)\ttotal: 1m 24s\tremaining: 20.5s\n",
      "170:\tlearn: 0.0296533\ttest: 0.0313270\tbest: 0.0313328 (167)\ttotal: 1m 29s\tremaining: 15.3s\n",
      "180:\tlearn: 0.0304022\ttest: 0.0327517\tbest: 0.0327517 (180)\ttotal: 1m 35s\tremaining: 9.99s\n",
      "190:\tlearn: 0.0311698\ttest: 0.0346041\tbest: 0.0346041 (190)\ttotal: 1m 40s\tremaining: 4.72s\n",
      "199:\tlearn: 0.0317386\ttest: 0.0358245\tbest: 0.0358245 (199)\ttotal: 1m 44s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0358244774\n",
      "bestIteration = 199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # 加载数据\n",
    "    # df = pd.read_parquet(f'{input_path}/train.parquet')   # 加载全部数据\n",
    "    df = pd.read_parquet(\n",
    "        f'{input_path}/train.parquet', \n",
    "        filters=[('partition_id', 'in', [4, 5, 6])]     # 只加载 partition_id 为 4, 5, 6 的数据\n",
    "    )\n",
    "    df = reduce_mem_usage(df, False)\n",
    "    df = df[df['date_id'] >= skip_dates].reset_index(drop=True)\n",
    "    \n",
    "    print(\"----------- Start to Load Dataset! -----------\")\n",
    "    # 准备数据集\n",
    "    dates = df['date_id'].unique()\n",
    "    train_data, valid_data = prepare_datasets(df, feature_names, dates, num_valid_dates)\n",
    "    print(\"----------- Dataset Loaded! -----------\")\n",
    "    \n",
    "    models = []\n",
    "    for model_type in models_toUse:\n",
    "        # 训练模型\n",
    "        model = model_dict[model_type]\n",
    "        trained_model = train_model(model, train_data, valid_data, model_type)\n",
    "        models.append(trained_model)\n",
    "        \n",
    "        # 保存模型\n",
    "        joblib.dump(trained_model, os.path.join(model_save_path, f'{model_type}.model'))\n",
    "        \n",
    "        # 清理内存\n",
    "        # del train_data\n",
    "        gc.collect()\n",
    "    \n",
    "else:\n",
    "    # 加载预训练模型\n",
    "    models = []\n",
    "    model_ckpt_path = \"./jsbaselinezyz/versions/1\"\n",
    "    for fold_idx in range(N_fold):\n",
    "        for model_type in ['lgb', 'xgb', 'cbt']:\n",
    "            model = joblib.load(f'{model_ckpt_path}/{model_type}_{fold_idx}.model')\n",
    "            models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n",
    "# Each batch of predictions (except the very first) must be returned within 10 minutes of the batch features being provided.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    predictions = test.select(\n",
    "        'row_id',\n",
    "        pl.lit(0.0).alias('responder_6'),\n",
    "    )\n",
    "    \n",
    "    feat = test[feature_names].to_numpy()\n",
    "    \n",
    "    pred = [model.predict(feat) for model in models]\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    \n",
    "    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n",
    "\n",
    "    # The predict function must return a DataFrame\n",
    "    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n",
    "    # with columns 'row_id', 'responer_6'\n",
    "    assert list(predictions.columns) == ['row_id', 'responder_6']\n",
    "    # and as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "inference_server = jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/Users/kyleee/code/project/kaggle_competition/data/test.parquet',\n",
    "            '/Users/kyleee/code/project/kaggle_competition/data/lags.parquet',\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
