{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    target_col = \"responder_6\"\n",
    "    lag_cols_original = [\"date_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n",
    "    lag_cols_rename = { f\"responder_{idx}\" : f\"responder_{idx}_lag_1\" for idx in range(9)}\n",
    "    valid_ratio = 0.05\n",
    "    start_dt = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_ids = [4, 5]\n",
    "train = pl.scan_parquet(\"../../data/train.parquet\").filter(pl.col(\"partition_id\").is_in(partition_ids))\n",
    "train = train.select(pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"id\"), pl.all()) # 增加 id 列\n",
    "train = train.with_columns((pl.col(CONFIG.target_col) * 2).cast(pl.Int32).alias(\"label\")) # 增加 label 列\n",
    "train = train.filter(pl.col(\"date_id\").gt(CONFIG.start_dt))   # 过滤掉前 1100 天的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = train.select(pl.col(CONFIG.lag_cols_original))\n",
    "lags = lags.rename(CONFIG.lag_cols_rename)\n",
    "lags = lags.with_columns(pl.col(\"date_id\") + 1)\n",
    "lags = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_df: (649528, 104)\n",
      "Last training date: 1019\n"
     ]
    }
   ],
   "source": [
    "processed_train = train.join(lags, on=[\"date_id\", \"symbol_id\"],  how=\"left\")\n",
    "train_df = processed_train.collect()\n",
    "print(f'shape of train_df: {train_df.shape}')\n",
    "if train_df.shape[0] == 0:\n",
    "    raise ValueError(\"The train dataframe is empty. Please check the input data.\")\n",
    "\n",
    "# 获取到验证集\n",
    "len_train = train_df.shape[0]\n",
    "valid_records = int(len_train * CONFIG.valid_ratio)\n",
    "len_of_val = len_train - valid_records\n",
    "if len_of_val >= len_train:\n",
    "    raise IndexError(f\"Index {len_of_val} is out of bounds. The dataset has only {len_train} rows.\")\n",
    "\n",
    "# 获取最后一个训练集日期\n",
    "# 使用 row 方法获取第 len_ofl_mdl 行的数据，注意返回的是元组\n",
    "last_train_dt = train_df.row(len_of_val)[train_df.columns.index(\"date_id\")]\n",
    "print(f\"Last training date: {last_train_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_train =649528\n",
      "len_ofl_vali =617052\n",
      "\n",
      "---> Last offline train date =1019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_tr_dt  = train_df.select(pl.col(\"date_id\")).row(len_of_val)[0]\n",
    "print(f\"len_train ={len_train}\")\n",
    "print(f\"len_ofl_vali ={len_of_val}\")\n",
    "print(f\"\\n---> Last offline train date ={last_tr_dt}\\n\")\n",
    "\n",
    "training_data = processed_train.filter(pl.col(\"date_id\").le(last_tr_dt))\n",
    "validation_data = processed_train.filter(pl.col(\"date_id\").gt(last_tr_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'date_id', 'time_id', 'symbol_id', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_6', 'responder_7', 'responder_8', 'partition_id', 'label', 'responder_0_lag_1', 'responder_1_lag_1', 'responder_2_lag_1', 'responder_3_lag_1', 'responder_4_lag_1', 'responder_5_lag_1', 'responder_6_lag_1', 'responder_7_lag_1', 'responder_8_lag_1']\n",
      "['id', 'date_id', 'time_id', 'symbol_id', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'responder_0', 'responder_1', 'responder_2', 'responder_3', 'responder_4', 'responder_5', 'responder_6', 'responder_7', 'responder_8', 'partition_id', 'label', 'responder_0_lag_1', 'responder_1_lag_1', 'responder_2_lag_1', 'responder_3_lag_1', 'responder_4_lag_1', 'responder_5_lag_1', 'responder_6_lag_1', 'responder_7_lag_1', 'responder_8_lag_1']\n"
     ]
    }
   ],
   "source": [
    "print(training_data.collect().columns)\n",
    "print(validation_data.collect().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.collect().write_parquet(f\"training.parquet\", partition_by = \"date_id\",)\n",
    "validation_data.collect().write_parquet(\"validation.parquet\", partition_by = \"date_id\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
